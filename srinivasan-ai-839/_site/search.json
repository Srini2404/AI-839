[
  {
    "objectID": "reference/train_model.html",
    "href": "reference/train_model.html",
    "title": "train_model",
    "section": "",
    "text": "train_model\ntrain_model(X_train, Y_train)\nTrains a RandomForestClassifier model using the provided training data.\nParameters: X_train (pd.DataFrame): The training features. Y_train (pd.Series): The training target.\nReturns: RandomForestClassifier: The trained RandomForestClassifier model."
  },
  {
    "objectID": "reference/preprocess_data.html",
    "href": "reference/preprocess_data.html",
    "title": "preprocess_data",
    "section": "",
    "text": "preprocess_data\npreprocess_data(df)\nConverts all object columns in the given DataFrame to categorical values using one-hot encoding.\nParameters: df (pd.DataFrame): A pandas DataFrame containing the data to be processed.\nReturns: pd.DataFrame: A pandas DataFrame with object columns converted to categorical values."
  },
  {
    "objectID": "reference/evaluate_model.html",
    "href": "reference/evaluate_model.html",
    "title": "evaluate_model",
    "section": "",
    "text": "evaluate_model\nevaluate_model(model, X_test, Y_test)\nEvaluates the trained model using the testing data and logs the accuracy.\nParameters: model (RandomForestClassifier): The trained model to be evaluated. X_test (pd.DataFrame): The testing features. Y_test (pd.Series): The testing target.\nReturns: None"
  },
  {
    "objectID": "notebooks/ModelTraining.html",
    "href": "notebooks/ModelTraining.html",
    "title": "Documentation for HW03",
    "section": "",
    "text": "from sklearn.metrics import accuracy_score, r2_score\nfrom sklearn.model_selection import train_test_split\n\n\n%load_ext kedro.ipython\n\nThe kedro.ipython extension is already loaded. To reload it, use:\n  %reload_ext kedro.ipython\n\n\n\ncatalog\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 catalog                                                                                      │\n│   2                                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'catalog' is not defined\n\n\n\n\ndataset = catalog.load(\"preprocessed_data\")\n\n[09/01/24 12:49:34] INFO     Loading data from preprocessed_data (CSVDataset)...                data_catalog.py:539\n\n\n\n\ndataset.head()\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nduration\ncredit_amount\ninstallment_commitment\nresidence_since\nage\nexisting_credits\nnum_dependents\nX_1\nX_2\n...\njob_high qualif/self emp/mgmt\njob_skilled\njob_unemp/unskilled non res\njob_unskilled resident\nown_telephone_none\nown_telephone_yes\nforeign_worker_no\nforeign_worker_yes\nhealth_status_bad\nhealth_status_good\n\n\n\n\n0\n0\n15\n2186.0\n1\n4\n33\n1\n1\n4.824403\n706.785635\n...\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n1\n1\n36\n7855.0\n4\n2\n25\n2\n1\n26.076695\n294.937039\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n2\n2\n24\n5103.0\n3\n3\n47\n3\n1\n51.868190\n1511.681913\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n3\n18\n2169.0\n4\n2\n28\n1\n1\n70.107094\n178.935446\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n4\n30\n6350.0\n4\n4\n31\n1\n1\n73.460211\n1213.202284\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n5 rows × 78 columns\n\n\n\n\ndataset.columns\n\n\n\n\n\nIndex(['Unnamed: 0', 'duration', 'credit_amount', 'installment_commitment',\n       'residence_since', 'age', 'existing_credits', 'num_dependents', 'X_1',\n       'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7', 'X_8', 'X_9', 'X_10', 'X_11',\n       'X_12', 'X_13', 'y', 'checking_status_0&lt;=X&lt;200', 'checking_status_&lt;0',\n       'checking_status_&gt;=200', 'checking_status_no checking',\n       'credit_history_all paid',\n       'credit_history_critical/other existing credit',\n       'credit_history_delayed previously', 'credit_history_existing paid',\n       'credit_history_no credits/all paid', 'purpose_business',\n       'purpose_domestic appliance', 'purpose_education',\n       'purpose_furniture/equipment', 'purpose_new car', 'purpose_other',\n       'purpose_radio/tv', 'purpose_repairs', 'purpose_retraining',\n       'purpose_used car', 'savings_status_100&lt;=X&lt;500',\n       'savings_status_500&lt;=X&lt;1000', 'savings_status_&lt;100',\n       'savings_status_&gt;=1000', 'savings_status_no known savings',\n       'employment_1&lt;=X&lt;4', 'employment_4&lt;=X&lt;7', 'employment_&lt;1',\n       'employment_&gt;=7', 'employment_unemployed',\n       'personal_status_female div/dep/mar', 'personal_status_male div/sep',\n       'personal_status_male mar/wid', 'personal_status_male single',\n       'other_parties_co applicant', 'other_parties_guarantor',\n       'other_parties_none', 'property_magnitude_car',\n       'property_magnitude_life insurance',\n       'property_magnitude_no known property',\n       'property_magnitude_real estate', 'other_payment_plans_bank',\n       'other_payment_plans_none', 'other_payment_plans_stores',\n       'housing_for free', 'housing_own', 'housing_rent',\n       'job_high qualif/self emp/mgmt', 'job_skilled',\n       'job_unemp/unskilled non res', 'job_unskilled resident',\n       'own_telephone_none', 'own_telephone_yes', 'foreign_worker_no',\n       'foreign_worker_yes', 'health_status_bad', 'health_status_good'],\n      dtype='object')\n\n\n\n\ndataset.drop(columns=[\"Unnamed: 0\"], inplace=True)\ndataset.columns\n\n\n\n\n\nIndex(['duration', 'credit_amount', 'installment_commitment',\n       'residence_since', 'age', 'existing_credits', 'num_dependents', 'X_1',\n       'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7', 'X_8', 'X_9', 'X_10', 'X_11',\n       'X_12', 'X_13', 'y', 'checking_status_0&lt;=X&lt;200', 'checking_status_&lt;0',\n       'checking_status_&gt;=200', 'checking_status_no checking',\n       'credit_history_all paid',\n       'credit_history_critical/other existing credit',\n       'credit_history_delayed previously', 'credit_history_existing paid',\n       'credit_history_no credits/all paid', 'purpose_business',\n       'purpose_domestic appliance', 'purpose_education',\n       'purpose_furniture/equipment', 'purpose_new car', 'purpose_other',\n       'purpose_radio/tv', 'purpose_repairs', 'purpose_retraining',\n       'purpose_used car', 'savings_status_100&lt;=X&lt;500',\n       'savings_status_500&lt;=X&lt;1000', 'savings_status_&lt;100',\n       'savings_status_&gt;=1000', 'savings_status_no known savings',\n       'employment_1&lt;=X&lt;4', 'employment_4&lt;=X&lt;7', 'employment_&lt;1',\n       'employment_&gt;=7', 'employment_unemployed',\n       'personal_status_female div/dep/mar', 'personal_status_male div/sep',\n       'personal_status_male mar/wid', 'personal_status_male single',\n       'other_parties_co applicant', 'other_parties_guarantor',\n       'other_parties_none', 'property_magnitude_car',\n       'property_magnitude_life insurance',\n       'property_magnitude_no known property',\n       'property_magnitude_real estate', 'other_payment_plans_bank',\n       'other_payment_plans_none', 'other_payment_plans_stores',\n       'housing_for free', 'housing_own', 'housing_rent',\n       'job_high qualif/self emp/mgmt', 'job_skilled',\n       'job_unemp/unskilled non res', 'job_unskilled resident',\n       'own_telephone_none', 'own_telephone_yes', 'foreign_worker_no',\n       'foreign_worker_yes', 'health_status_bad', 'health_status_good'],\n      dtype='object')\n\n\n\n\nX = dataset.copy()\nX.drop(columns=\"y\", axis=1, inplace=True)\n\n\nY = dataset[\"y\"]\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nmodel1 = GradientBoostingClassifier()\n\n\nmodel.fit(X_train, Y_train)\nmodel1.fit(X_train, Y_train)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n\nY_predict = model.predict(X_test)\nY_predict1 = model1.predict(X_test)\n\n\nprint(accuracy_score(Y_test, Y_predict))\n# print(accuracy_score(Y_test,Y_predict1))\n\n0.96"
  },
  {
    "objectID": "notebooks/hw01.html",
    "href": "notebooks/hw01.html",
    "title": "Project Card",
    "section": "",
    "text": "The purpose of Project Cards is two folds. During development, it helps the developer think about the problem in a structured way w.r. t framing the problem, assessing the business value, viability, and many other aspects.\nIt can also serve as a document giving a high level overview of the system developed and deployed. With proper versioning, one can also see the evolution of the problem. It is meant to be a high level document and as details emerge, documents such Model Cards and Data Cards can be linked.\nThe following are the different sections of the Project Cards.\nThis notebook uses tags to render the output. Each call has a tag. There are three tags: objective, instruction, response. The cell with objective tag explains the purpose of this project card. Cells with instruction tag, are the key sections of the document that must be filled. A cell following immediately will have a tag response. You only fill the cell with response tag. DO NOT MODIFY the cells with tag instruction. Of course, feel free to modify to your needs. Once the format is agreed upon, stick to it."
  },
  {
    "objectID": "notebooks/hw01.html#background",
    "href": "notebooks/hw01.html#background",
    "title": "Project Card",
    "section": "Background",
    "text": "Background\nThe real-estate business is evolving and challenging. Realtors are looking to attract customers to sell their houses and customers on the other hand want to get the best for what they pay."
  },
  {
    "objectID": "notebooks/hw01.html#problem",
    "href": "notebooks/hw01.html#problem",
    "title": "Project Card",
    "section": "Problem",
    "text": "Problem\nWhat is the problem being solved?\nHelp users or home buyers in buying houses and recommending them the best ones for the money they pay."
  },
  {
    "objectID": "notebooks/hw01.html#customer",
    "href": "notebooks/hw01.html#customer",
    "title": "Project Card",
    "section": "Customer",
    "text": "Customer\nWho it is for? Is that a user or a beneficiary? What is the problem being solved? Who it is for?\nThe solution is for a laymen buyer who wants to buy a house."
  },
  {
    "objectID": "notebooks/hw01.html#value-proposition",
    "href": "notebooks/hw01.html#value-proposition",
    "title": "Project Card",
    "section": "Value Proposition",
    "text": "Value Proposition\nWhy it needs to be solved?\nHaving a place to live in is next to essential, it is important to assist them in doing it."
  },
  {
    "objectID": "notebooks/hw01.html#product",
    "href": "notebooks/hw01.html#product",
    "title": "Project Card",
    "section": "Product",
    "text": "Product\nHow does the solution look like? It is more of the experience, rather how it will be developed.\nIt will be a user friendly webpage that will take in the user’s input and provide the user with the recommendations."
  },
  {
    "objectID": "notebooks/hw01.html#objectives",
    "href": "notebooks/hw01.html#objectives",
    "title": "Project Card",
    "section": "Objectives",
    "text": "Objectives\nBreakdown the product into key (business) objectives that need to be delivered? SMART Goals is useful to frame\nTo deliver a housing recommendation application using machine learning, we can set the following SMART objectives:\n\nSpecific: Develop a machine learning model that recommends houses based on user preferences and budget.\nMeasurable: Achieve an accuracy of at least 80% in predicting user preferences and recommending suitable houses.\nAchievable: Collect a dataset of at least 10,000 houses with relevant features such as location, price, size, amenities, etc.\nRelevant: Ensure that the recommended houses align with the user’s preferences, budget, and location.\nTime-bound: Complete the development and testing of the housing recommendation application within 6 months.\n\nBy setting these SMART objectives, we can ensure a clear and measurable path towards delivering a successful housing recommendation application using machine learning."
  },
  {
    "objectID": "notebooks/hw01.html#risks-challenges",
    "href": "notebooks/hw01.html#risks-challenges",
    "title": "Project Card",
    "section": "Risks & Challenges",
    "text": "Risks & Challenges\nWhat are the challenges one can face and ways to overcome?\nSome potential risks and challenges that we might face while creating the housing recommendation application are :\n\nData Availability: Obtaining a comprehensive and reliable dataset of houses with relevant features such as location, price, size, amenities, etc. can be challenging. Limited or incomplete data may affect the accuracy and effectiveness of the recommendations.\nData Quality: Ensuring the quality and accuracy of the collected data is crucial. Inaccurate or inconsistent data can lead to biased recommendations and unreliable predictions.\nModel Complexity: Developing a machine learning model that accurately predicts user preferences and recommends suitable houses can be complex. Choosing the right algorithms, feature engineering, and model optimization require expertise and careful consideration.\nUser Adoption: Convincing users to adopt and trust the housing recommendation application may pose a challenge. Users may have concerns about privacy, data security, and the reliability of the recommendations.\nScalability: Designing the application to handle a large number of users and a growing dataset can be challenging. Ensuring that the application remains responsive and efficient as the user base and data volume increase is essential.\nInterpretability: Interpreting and explaining the recommendations generated by the machine learning model can be difficult. Users may require transparency and understanding of how the recommendations are generated to build trust in the system.\nRegulatory Compliance: Adhering to data protection and privacy regulations can be a challenge. Ensuring that user data is handled securely and in compliance with applicable laws and regulations is essential.\nModel Maintenance: As the real estate market evolves, the machine learning model needs to be regularly updated and maintained to reflect changing trends and user preferences. Keeping the model up-to-date and relevant can be a continuous effort.\nUser Feedback and Iteration: Incorporating user feedback and continuously improving the application based on user needs and preferences can be challenging. Balancing user expectations, business goals, and technical constraints requires effective communication and agile development practices.\nTechnical Infrastructure: Setting up and maintaining the necessary technical infrastructure, including servers, databases, and deployment pipelines, can be complex. Ensuring the application is robust, scalable, and reliable requires careful planning and implementation.\n\nIt is important to anticipate these risks and challenges and have mitigation strategies in place to address them effectively during the development and deployment of the housing recommendation application."
  },
  {
    "objectID": "notebooks/hw01.html#task",
    "href": "notebooks/hw01.html#task",
    "title": "Project Card",
    "section": "Task",
    "text": "Task\nWhat type of prediction problem is this? Link Model Card when sufficient details become available (start small but early)\nThis is more of a recommendation problem that will recommend users the houses that intreset them and which closely aligns with their requirements."
  },
  {
    "objectID": "notebooks/hw01.html#metrics",
    "href": "notebooks/hw01.html#metrics",
    "title": "Project Card",
    "section": "Metrics",
    "text": "Metrics\nHow will the solution be evaluated - What are the ML metrics? What are the business metrics? Link Model Card when sufficient details become available (start small but early)\nTo evaluate the performance of the ML model for the recommendation system for housing, we can use the following metrics:\n\nPrecision: Precision measures the proportion of correctly recommended houses out of all the houses recommended. It helps assess the accuracy of the recommendations and the model’s ability to avoid false positives.\nRecall: Recall measures the proportion of correctly recommended houses out of all the houses that should have been recommended. It helps assess the model’s ability to avoid false negatives and capture all relevant houses.\nF1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure of the model’s performance, taking into account both precision and recall.\nMean Average Precision (MAP): MAP calculates the average precision across different levels of recommendation. It considers the order of the recommended houses and rewards models that rank more relevant houses higher.\nNormalized Discounted Cumulative Gain (NDCG): NDCG measures the quality of the ranking of the recommended houses. It takes into account both the relevance of the recommended houses and their position in the ranking.\nHit Rate: Hit rate measures the proportion of users for whom at least one relevant house was recommended. It helps assess the coverage of the recommendation system.\nMean Reciprocal Rank (MRR): MRR calculates the average reciprocal rank of the first relevant house in the recommendation list. It provides a measure of the model’s ability to rank the most relevant houses higher.\nUser Satisfaction: User satisfaction can be measured through surveys or feedback from users. It helps assess the overall user experience and satisfaction with the recommended houses.\n\nThese metrics can be calculated using a holdout or cross-validation approach, where a subset of the data is used for evaluation. It is important to consider both the ML metrics and the business metrics to ensure that the recommendation system meets the needs and expectations of the users."
  },
  {
    "objectID": "notebooks/hw01.html#evaluation",
    "href": "notebooks/hw01.html#evaluation",
    "title": "Project Card",
    "section": "Evaluation",
    "text": "Evaluation\nHow will the solution be evaluated (process)? Link Model Card when sufficient details become available (start small but early)\nyour response"
  },
  {
    "objectID": "notebooks/hw01.html#data",
    "href": "notebooks/hw01.html#data",
    "title": "Project Card",
    "section": "Data",
    "text": "Data\nWhat type of data is needed? How will it be collected - for training and for continuous improvement? Link Data Cards when sufficient details become available (start small but early)\nTo train a housing recommendation model, we will need the following data:\n\nHouse Descriptions: This includes information such as the number of bedrooms, bathrooms, square footage, location, price, and any additional features or characteristics of the house.\nHouse Images: Images of the houses can be used to extract visual features and provide a more comprehensive understanding of the property.\nProximity Data: Data on nearby amenities such as schools, colleges, marketplaces, hospitals, parks, and transportation facilities. This information helps assess the convenience and accessibility of the location.\nAmenities Data: Details about the amenities available within the house or the housing complex, such as swimming pools, gyms, parking spaces, security systems, and other facilities.\n\nCollecting this data can be done through various sources, including real estate websites, property listings, public APIs, and data scraping techniques. It is important to ensure the data is accurate, up-to-date, and representative of the target housing market.\nAdditionally, data preprocessing steps may be required to clean and transform the data into a suitable format for training the recommendation model. This may involve handling missing values, normalizing numerical features, encoding categorical variables, and performing feature engineering to extract relevant information.\nBy gathering and preparing this data, we can build a robust housing recommendation system that considers various factors and preferences to provide accurate and personalized recommendations to users."
  },
  {
    "objectID": "notebooks/hw01.html#plan-roadmap",
    "href": "notebooks/hw01.html#plan-roadmap",
    "title": "Project Card",
    "section": "Plan/ Roadmap",
    "text": "Plan/ Roadmap\nProvide problem break-up, tentative timelines and deliverables? Use PACT format if SMART is not suitable.\n\nProblem Understanding and Data Collection (2 weeks)\n\nDefine the problem statement and objectives of the housing recommendation system.\nGather relevant data sources such as house descriptions, images, proximity data, and amenities data.\nPerform data cleaning and preprocessing to ensure data quality and consistency.\n\nExploratory Data Analysis (1 week)\n\nAnalyze the collected data to gain insights into the distribution and characteristics of the houses.\nIdentify any patterns or correlations between features that may be useful for recommendation.\n\nFeature Engineering (2 weeks)\n\nExtract and engineer relevant features from the collected data.\nConsider factors such as location, price, size, amenities, and proximity to create informative features.\n\nModel Development (4 weeks)\n\nSelect an appropriate machine learning algorithm for the recommendation task.\nTrain the model using the prepared dataset and evaluate its performance.\nFine-tune the model parameters to optimize its accuracy and effectiveness.\n\nUser Interface Design (3 weeks)\n\nDesign a user-friendly webpage or application interface for users to input their preferences.\nImplement interactive features such as filters, sliders, and search functionalities for a seamless user experience.\n\nIntegration and Testing (2 weeks)\n\nIntegrate the trained recommendation model with the user interface.\nConduct thorough testing to ensure the system functions correctly and provides accurate recommendations.\n\nDeployment and Monitoring (1 week)\n\nDeploy the housing recommendation system to a production environment.\nSet up monitoring and logging mechanisms to track system performance and user interactions.\n\nContinuous Improvement (Ongoing)\n\nGather user feedback and incorporate it into the system to enhance recommendation accuracy.\nMonitor system performance and make necessary updates to adapt to changing user preferences and market trends.\n\n\nPlease note that the timeline provided is just a rough estimate and can vary depending on the available resources."
  },
  {
    "objectID": "notebooks/hw01.html#continuous-improvement",
    "href": "notebooks/hw01.html#continuous-improvement",
    "title": "Project Card",
    "section": "Continuous Improvement",
    "text": "Continuous Improvement\nHow will the system/model will improve? Provide a plan and means.\nTo continuously improve the system/model, we can follow a plan that includes the following steps:\n\nCollect User Feedback: Encourage users to provide feedback on the recommendations they receive. This can be done through surveys, ratings, or comments. User feedback is valuable in understanding the strengths and weaknesses of the system and identifying areas for improvement.\nMonitor Performance Metrics: Continuously monitor the performance metrics of the recommendation system/model. This includes metrics such as accuracy, precision, recall, and user satisfaction. By tracking these metrics, we can identify any deviations or drops in performance and take corrective actions.\nAnalyze User Behavior: Analyze user behavior data to gain insights into how users interact with the system and the recommendations they find most useful. This can be done by tracking user clicks, dwell time, and engagement metrics. Understanding user preferences and patterns can help in refining the recommendation algorithms.\nIncorporate New Data: Regularly update the dataset used for training the recommendation system/model. This can involve collecting new data from various sources, such as real estate listings, user feedback, and external data providers. By incorporating new data, the system/model can adapt to changing trends and preferences.\nExperiment with Algorithms: Explore and experiment with different recommendation algorithms and techniques. This can include trying out collaborative filtering, content-based filtering, hybrid approaches, or even advanced techniques like deep learning. By experimenting with different algorithms, we can identify the ones that provide the best recommendations for our users.\nA/B Testing: Conduct A/B testing to evaluate the performance of different versions or variations of the recommendation system/model. This involves randomly assigning users to different groups and comparing the performance metrics between the groups. A/B testing helps in validating the effectiveness of new features or changes before deploying them to all users.\nRegular Model Updates: Keep the recommendation model up-to-date by retraining it periodically with new data. This ensures that the model stays relevant and accurate over time. Consider implementing automated pipelines for data collection, preprocessing, and model training to streamline the update process.\nStay Updated with Research: Stay updated with the latest research and advancements in the field of recommendation systems. This can involve reading research papers, attending conferences, and participating in online communities. By staying informed, we can leverage new techniques and insights to enhance the system/model.\n\nBy following this plan and continuously iterating on the system/model based on user feedback and performance metrics, we can ensure that the recommendation system/model improves over time and provides valuable and personalized recommendations to users."
  },
  {
    "objectID": "notebooks/hw01.html#resources",
    "href": "notebooks/hw01.html#resources",
    "title": "Project Card",
    "section": "Resources",
    "text": "Resources\nWhat resources are needed? Estimate the cost!\nTo train the recommendation system model, the following resources are needed:\n\nData: A comprehensive dataset of houses with relevant features such as location, price, size, amenities, etc. This data can be collected from real estate websites, property listings, public APIs, and data scraping techniques.\nHouse Descriptions: Information about the number of bedrooms, bathrooms, square footage, and any additional features or characteristics of the house.\nHouse Images: Images of the houses to extract visual features and provide a more comprehensive understanding of the property.\nProximity Data: Data on nearby amenities such as schools, colleges, marketplaces, hospitals, parks, and transportation facilities. This information helps assess the convenience and accessibility of the location.\nAmenities Data: Details about the amenities available within the house or the housing complex, such as swimming pools, gyms, parking spaces, security systems, and other facilities.\nComputational Resources: Sufficient computational resources, such as a powerful CPU or GPU, to handle the training process efficiently.\nMachine Learning Libraries: Libraries such as scikit-learn, TensorFlow, or PyTorch to implement and train the recommendation system model.\nDevelopment Environment: A Jupyter Notebook or any other development environment to write and execute the code for training the model.\nTime and Effort: Sufficient time and effort to collect and preprocess the data, design and implement the model, and fine-tune its parameters.\nDomain Knowledge: A good understanding of the real estate market and the factors that influence housing preferences and recommendations.\n\n\nHuman Resources\nwhat type of team and strength needed?\nTo build this application, you would need a team with the following roles and strengths:\n\nData Scientist: A data scientist with expertise in machine learning and recommendation systems would be essential for developing the housing recommendation model. They should have experience in data preprocessing, feature engineering, model selection, and evaluation.\nSoftware Engineer: A software engineer with proficiency in web development would be required to build the user-friendly webpage for the application. They should have knowledge of front-end technologies such as HTML, CSS, and JavaScript, as well as back-end technologies like Python and web frameworks such as Flask or Django.\nDatabase Administrator: A database administrator would be responsible for designing and maintaining the database that stores the housing data. They should have expertise in database management systems like MySQL, PostgreSQL, or MongoDB.\nUI/UX Designer: A UI/UX designer would be needed to create an intuitive and visually appealing user interface for the application. They should have skills in user research, wireframing, prototyping, and usability testing.\nDomain Expert: A domain expert in the real estate industry would be valuable for understanding user requirements, defining relevant features, and ensuring the recommendations align with market trends and preferences.\nProject Manager: A project manager would be responsible for overseeing the development process, coordinating tasks, managing timelines, and ensuring effective communication among team members.\nQuality Assurance Engineer: A quality assurance engineer would be required to test the application, identify and report any bugs or issues, and ensure the application meets the desired quality standards.\nData Engineer: A data engineer would be responsible for data collection, data cleaning, and data integration tasks. They should have knowledge of data extraction techniques, data pipelines, and data storage technologies.\nDevOps Engineer: A DevOps engineer would be needed to set up and manage the deployment infrastructure, automate the deployment process, and ensure the application is scalable, secure, and reliable.\nCommunication and Collaboration Skills: Strong communication and collaboration skills are essential for effective teamwork, as team members need to work together, share ideas, and align their efforts towards a common goal.\n\n\n\nCompute Resources\nWhat type of compute resources needed to train and serve?\nTo train and serve the housing recommendation model, you would need the following computer resources:\n\nHardware: A computer with sufficient computational power is required for training the model. This can include a high-performance CPU or GPU, depending on the complexity of the model and the size of the dataset. GPUs are particularly useful for accelerating the training process, especially for deep learning models.\nMemory: Sufficient memory (RAM) is necessary to handle the data and model during training. The amount of memory required depends on the size of the dataset and the complexity of the model. Larger datasets and more complex models generally require more memory.\nStorage: Adequate storage space is needed to store the dataset, model files, and any other necessary resources. The size of the storage depends on the size of the dataset and the number of models being trained and served.\nSoftware: You will need software tools and libraries for training and serving the model. This can include machine learning frameworks such as scikit-learn, TensorFlow, or PyTorch, as well as any additional libraries or packages required for data preprocessing, model evaluation, and deployment.\nDevelopment Environment: A Jupyter Notebook or any other development environment is needed to write and execute the code for training the model. This environment should have the necessary software dependencies installed.\nNetworking: A stable internet connection is required for downloading datasets, accessing external APIs or services, and serving the model to users.\nScalability: If you anticipate a large number of users or a growing dataset, you may need to consider scalable infrastructure options such as cloud computing platforms or distributed computing frameworks to handle the increased workload."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Documentation for HW03",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notebooks/data_exploration.html",
    "href": "notebooks/data_exploration.html",
    "title": "Documentation for HW03",
    "section": "",
    "text": "%load_ext kedro.ipython\n\n[09/03/24 00:14:50] INFO     Using                                                                  __init__.py:249\n                             'C:\\Users\\Admin\\miniconda3\\envs\\recsys\\Lib\\site-packages\\kedro\\framewo                \n                             rk\\project\\rich_logging.yml' as logging configuration.                                \n\n\n\n                    INFO     Registered line magic '%reload_kedro'                                   __init__.py:58\n\n\n\n                    INFO     Registered line magic '%load_node'                                      __init__.py:60\n\n\n\n                    INFO     Resolved project path as:                                              __init__.py:175\n                             C:\\Users\\Admin\\Desktop\\Semester_7\\MLOps\\AI-839\\srinivasan-ai-839.                     \n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n[09/03/24 00:14:53] WARNING  C:\\Users\\Admin\\miniconda3\\envs\\recsys\\Lib\\site-packages\\kedro_viz\\__in warnings.py:112\n                             it__.py:13: KedroVizPythonVersionWarning: Please be advised that Kedro                \n                             Viz is not yet fully                                                                  \n                                     compatible with the Python version you are currently using.                   \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n[09/03/24 00:14:54] INFO     Kedro is sending anonymous usage data with the sole purpose of improving plugin.py:233\n                             the product. No personal data or IP addresses are stored on our side. If              \n                             you want to opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK`              \n                             environment variables, or create a `.telemetry` file in the current                   \n                             working directory with the contents `consent: false`. Read more at                    \n                             https://docs.kedro.org/en/stable/configuration/telemetry.html                         \n\n\n\n[09/03/24 00:14:56] INFO     Kedro project Srinivasan-ai-839                                        __init__.py:141\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:142\n                             'pipelines'                                                                           \n\n\n\n[09/03/24 00:15:00] INFO     Registered line magic 'run_viz'                                        __init__.py:148\n\n\n\n\ncatalog\n\n\n\n\n\n{'dataset_id_214': \"kedro_datasets.pandas.csv_dataset.CSVDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/Semester_7/MLOps/AI-839/srinivasan-ai-839/data/01_raw/dataset_id_214.csv'), \"\n                   \"protocol='file', load_args={}, save_args={'index': False})\",\n 'preprocessed_data': \"kedro_datasets.pandas.csv_dataset.CSVDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/Semester_7/MLOps/AI-839/srinivasan-ai-839/data/02_modelinput/preprocessed_data.csv'), \"\n                      \"protocol='file', load_args={}, save_args={'index': \"\n                      'True})',\n 'classifier_model': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/Semester_7/MLOps/AI-839/srinivasan-ai-839/data/06_models/classifier_model.pkl'), \"\n                     \"backend='pickle', protocol='file', load_args={}, \"\n                     'save_args={}, version=Version(load=None, '\n                     \"save='2024-09-02T18.44.54.260Z'))\",\n 'parameters': \"kedro.io.memory_dataset.MemoryDataset(data='&lt;dict&gt;')\",\n 'params:model_options': \"kedro.io.memory_dataset.MemoryDataset(data='&lt;dict&gt;')\",\n 'params:model_options.random_state': \"kedro.io.memory_dataset.MemoryDataset(data='&lt;int&gt;')\"}\n\n\n\n\ndataset = catalog.load(\"dataset_id_214\")\n\n[09/03/24 00:15:07] INFO     Loading data from dataset_id_214 (CSVDataset)...                   data_catalog.py:539\n\n\n\n\ndataset.dtypes\n\n\n\n\n\nchecking_status            object\nduration                    int64\ncredit_history             object\npurpose                    object\ncredit_amount             float64\nsavings_status             object\nemployment                 object\ninstallment_commitment      int64\npersonal_status            object\nother_parties              object\nresidence_since             int64\nproperty_magnitude         object\nage                         int64\nother_payment_plans        object\nhousing                    object\nexisting_credits            int64\njob                        object\nnum_dependents              int64\nown_telephone              object\nforeign_worker             object\nhealth_status              object\nX_1                       float64\nX_2                       float64\nX_3                       float64\nX_4                       float64\nX_5                       float64\nX_6                       float64\nX_7                       float64\nX_8                       float64\nX_9                       float64\nX_10                      float64\nX_11                      float64\nX_12                      float64\nX_13                      float64\ny                            bool\ndtype: object\n\n\n\ndataset.head()\n\n\n\n\n\n\n\n\n\n\n\nchecking_status\nduration\ncredit_history\npurpose\ncredit_amount\nsavings_status\nemployment\ninstallment_commitment\npersonal_status\nother_parties\n...\nX_5\nX_6\nX_7\nX_8\nX_9\nX_10\nX_11\nX_12\nX_13\ny\n\n\n\n\n0\nno checking\n15\nexisting paid\nfurniture/equipment\n2186.0\nno known savings\n4&lt;=X&lt;7\n1\nfemale div/dep/mar\nnone\n...\n0.048244\n0.355725\n0.909741\n0.405124\n0.260767\n0.103618\n0.406376\n0.017897\n0.518682\nTrue\n\n\n1\nno checking\n36\ncritical/other existing credit\nnew car\n7855.0\n&lt;100\n1&lt;=X&lt;4\n4\nfemale div/dep/mar\nnone\n...\n0.848429\n0.167300\n0.525345\n0.701071\n0.032609\n0.347617\n0.805072\n0.734602\n0.665720\nFalse\n\n\n2\nno checking\n24\ncritical/other existing credit\nradio/tv\n5103.0\n&lt;100\n&lt;1\n3\nmale mar/wid\nnone\n...\n0.135713\n0.185599\n0.992303\n0.778875\n0.236489\n0.317112\n0.751187\n0.080140\n0.272325\nTrue\n\n\n3\nno checking\n18\ndelayed previously\nbusiness\n2169.0\n&lt;100\n1&lt;=X&lt;4\n4\nmale mar/wid\nnone\n...\n0.595960\n0.138928\n0.515338\n0.198236\n0.515988\n0.190489\n0.459742\n0.018711\n0.512552\nFalse\n\n\n4\n&lt;0\n30\nexisting paid\nfurniture/equipment\n6350.0\nno known savings\n&gt;=7\n4\nmale single\nnone\n...\n0.828696\n0.991522\n0.519209\n0.068189\n0.121944\n0.326375\n0.774543\n0.898108\n0.173491\nFalse\n\n\n\n\n5 rows × 35 columns\n\n\n\n\nimport pandas as pd\n\ndf = dataset.copy()\ndf = pd.get_dummies(df, drop_first=False)\n\n\ndataset.dtypes\n\n\n\n\n\nchecking_status            object\nduration                    int64\ncredit_history             object\npurpose                    object\ncredit_amount             float64\nsavings_status             object\nemployment                 object\ninstallment_commitment      int64\npersonal_status            object\nother_parties              object\nresidence_since             int64\nproperty_magnitude         object\nage                         int64\nother_payment_plans        object\nhousing                    object\nexisting_credits            int64\njob                        object\nnum_dependents              int64\nown_telephone              object\nforeign_worker             object\nhealth_status              object\nX_1                       float64\nX_2                       float64\nX_3                       float64\nX_4                       float64\nX_5                       float64\nX_6                       float64\nX_7                       float64\nX_8                       float64\nX_9                       float64\nX_10                      float64\nX_11                      float64\nX_12                      float64\nX_13                      float64\ny                            bool\ndtype: object\n\n\n\n# dataset[dataset['foreign_worker']=='no']\n\n\n\n\n\n\n\n\n\n\n\nchecking_status\nduration\ncredit_history\npurpose\ncredit_amount\nsavings_status\nemployment\ninstallment_commitment\npersonal_status\nother_parties\n...\nX_5\nX_6\nX_7\nX_8\nX_9\nX_10\nX_11\nX_12\nX_13\ny\n\n\n\n\n83\n&lt;0\n8\ncritical/other existing credit\nnew car\n3398.0\n&lt;100\n4&lt;=X&lt;7\n1\nmale single\nnone\n...\n0.314531\n0.838259\n0.064414\n0.675418\n0.046085\n0.786261\n0.188436\n0.198783\n0.610006\nTrue\n\n\n\n\n1 rows × 35 columns\n\n\n\n\ndf.dtypes\n\n\n\n\n\nduration                    int64\ncredit_amount             float64\ninstallment_commitment      int64\nresidence_since             int64\nage                         int64\n                           ...   \nown_telephone_yes            bool\nforeign_worker_no            bool\nforeign_worker_yes           bool\nhealth_status_bad            bool\nhealth_status_good           bool\nLength: 77, dtype: object\n\n\n\n\ndataset.dropna()\n\n\n\n\n\n\n\n\n\n\n\nchecking_status\nduration\ncredit_history\npurpose\ncredit_amount\nsavings_status\nemployment\ninstallment_commitment\npersonal_status\nother_parties\n...\nX_5\nX_6\nX_7\nX_8\nX_9\nX_10\nX_11\nX_12\nX_13\ny\n\n\n\n\n0\nno checking\n15\nexisting paid\nfurniture/equipment\n2186.0\nno known savings\n4&lt;=X&lt;7\n1\nfemale div/dep/mar\nnone\n...\n0.048244\n0.355725\n0.909741\n0.405124\n0.260767\n0.103618\n0.406376\n0.017897\n0.518682\nTrue\n\n\n1\nno checking\n36\ncritical/other existing credit\nnew car\n7855.0\n&lt;100\n1&lt;=X&lt;4\n4\nfemale div/dep/mar\nnone\n...\n0.848429\n0.167300\n0.525345\n0.701071\n0.032609\n0.347617\n0.805072\n0.734602\n0.665720\nFalse\n\n\n2\nno checking\n24\ncritical/other existing credit\nradio/tv\n5103.0\n&lt;100\n&lt;1\n3\nmale mar/wid\nnone\n...\n0.135713\n0.185599\n0.992303\n0.778875\n0.236489\n0.317112\n0.751187\n0.080140\n0.272325\nTrue\n\n\n3\nno checking\n18\ndelayed previously\nbusiness\n2169.0\n&lt;100\n1&lt;=X&lt;4\n4\nmale mar/wid\nnone\n...\n0.595960\n0.138928\n0.515338\n0.198236\n0.515988\n0.190489\n0.459742\n0.018711\n0.512552\nFalse\n\n\n4\n&lt;0\n30\nexisting paid\nfurniture/equipment\n6350.0\nno known savings\n&gt;=7\n4\nmale single\nnone\n...\n0.828696\n0.991522\n0.519209\n0.068189\n0.121944\n0.326375\n0.774543\n0.898108\n0.173491\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n0&lt;=X&lt;200\n24\nexisting paid\nnew car\n2718.0\n&lt;100\n1&lt;=X&lt;4\n3\nfemale div/dep/mar\nnone\n...\n0.882218\n0.883794\n0.233441\n0.150932\n0.499922\n0.434244\n0.140232\n0.187078\n0.141813\nFalse\n\n\n96\nno checking\n12\ncritical/other existing credit\neducation\n2096.0\n&lt;100\n4&lt;=X&lt;7\n2\nmale single\nnone\n...\n0.149197\n0.043498\n0.134882\n0.350446\n0.889813\n0.760545\n0.261638\n0.823732\n0.053394\nTrue\n\n\n97\nno checking\n36\ncritical/other existing credit\nnew car\n6614.0\n&lt;100\n&gt;=7\n4\nmale single\nnone\n...\n0.424718\n0.253165\n0.242619\n0.903971\n0.694778\n0.517654\n0.707042\n0.575040\n0.073795\nFalse\n\n\n98\nno checking\n12\nexisting paid\nradio/tv\n1493.0\n&lt;100\n&lt;1\n4\nfemale div/dep/mar\nnone\n...\n0.469677\n0.654652\n0.316479\n0.831883\n0.405854\n0.259628\n0.350143\n0.868693\n0.065413\nTrue\n\n\n99\nno checking\n18\ncritical/other existing credit\neducation\n1864.0\n100&lt;=X&lt;500\n1&lt;=X&lt;4\n4\nfemale div/dep/mar\nnone\n...\n0.557202\n0.300939\n0.362722\n0.684180\n0.774063\n0.283213\n0.436374\n0.226074\n0.129677\nFalse\n\n\n\n\n100 rows × 35 columns\n\n\n\n\nimport pandas as pd\n\n\ndef isTelephone(x: pd.Series) -&gt; pd.Series:\n    return x == \"yes\"\n\n\ndataset[\"own_telephone\"] = isTelephone(dataset[\"own_telephone\"])\n\n\ndataset[\"own_telephone\"]\n\n\n\n\n\n0     False\n1      True\n2      True\n3      True\n4     False\n      ...  \n95     True\n96    False\n97     True\n98    False\n99    False\nName: own_telephone, Length: 100, dtype: bool"
  },
  {
    "objectID": "notebooks/hw02.html",
    "href": "notebooks/hw02.html",
    "title": "Home Work 2",
    "section": "",
    "text": "using PyFME library to include details about meta information of the dataset."
  },
  {
    "objectID": "notebooks/hw02.html#dataset-overview",
    "href": "notebooks/hw02.html#dataset-overview",
    "title": "Home Work 2",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nThe Soybean dataset is a well-known benchmark dataset commonly used in machine learning tasks. It contains 683 instances and 35 attributes (excluding the target class). The dataset has 19 distinct classes, representing different types of soybean diseases.\n\nBasic Dataset Features:\n\nNumber of Instances: 683\nNumber of Attributes: 35\nNumber of Classes: 19"
  },
  {
    "objectID": "notebooks/hw02.html#meta-features-extracted",
    "href": "notebooks/hw02.html#meta-features-extracted",
    "title": "Home Work 2",
    "section": "Meta-Features Extracted",
    "text": "Meta-Features Extracted\nMeta-features provide insights into the characteristics of the dataset that could be used for tasks like meta-learning. Below are some of the meta-features extracted:\n\n\n\nFeature\nValue\n\n\n\n\nnum_instances\n683\n\n\nnum_attributes\n35\n\n\nnum_classes\n19\n\n\nattr_conc.mean\n0.097\n\n\nattr_conc.sd\n0.155\n\n\nbest_node.mean\n0.261\n\n\nbest_node.sd\n0.006\n\n\ncan_cor.mean\n0.945\n\n\ncan_cor.sd\n0.088\n\n\nclass_conc.mean\n0.553\n\n\nclass_conc.sd\n0.292\n\n\nclass_ent\n3.836\n\n\ncor.mean\n0.145\n\n\ncor.sd\n0.151\n\n\neigenvalues.mean\n0.145\n\n\neigenvalues.sd\n0.368\n\n\nfreq_class.mean\n0.053\n\n\nfreq_class.sd\n0.044\n\n\ngravity\n5.676\n\n\nkurtosis.mean\n12.651\n\n\nkurtosis.sd\n68.915\n\n\nleaves\n72\n\n\nleaves_branch.mean\n10.389\n\n\nleaves_branch.sd\n3.899\n\n\nleaves_per_class.mean\n0.053\n\n\nleaves_per_class.sd\n0.066\n\n\nmax.mean\n1.0\n\n\nmedian.mean\n0.263\n\n\nmedian.sd\n0.442\n\n\nmin.mean\n0.0\n\n\nmin.sd\n0.0\n\n\nnodes\n71\n\n\nnodes_per_attr\n0.717\n\n\none_nn.mean\n0.924\n\n\none_nn.sd\n0.053\n\n\nrandom_node.mean\n0.198\n\n\nrandom_node.sd\n0.029\n\n\ntree_depth.mean\n9.469\n\n\ntree_depth.sd\n4.036\n\n\nvar.mean\n0.145\n\n\nvar.sd\n0.076\n\n\nworst_node.mean\n0.132\n\n\nworst_node.sd\n0.001\n\n\n\n\nNote:\nSome meta-features like attr_ent.mean, attr_ent.sd, cat_to_num, mut_inf.mean, etc., were not available in this dataset (NaN values)."
  },
  {
    "objectID": "notebooks/hw02.html#code-to-extract-features",
    "href": "notebooks/hw02.html#code-to-extract-features",
    "title": "Home Work 2",
    "section": "Code to Extract Features",
    "text": "Code to Extract Features\nThe following code was used to load the Soybean dataset, extract basic dataset features, and compute meta-features using the PyMFE library:\n\nimport pandas as pd\nimport arff\nfrom pymfe.mfe import MFE\n\n# Load the ARFF dataset using liac-arff\nwith open(\"../../dataset_42_soybean.arff\", \"r\") as f:\n    dataset = arff.load(f)\n\n# Convert to a DataFrame\ndf = pd.DataFrame(dataset[\"data\"], columns=[attr[0] for attr in dataset[\"attributes\"]])\n\n# Extract basic dataset features\nnum_instances = df.shape[0]\nnum_attributes = df.shape[1] - 1  # Subtract 1 for the target class column\nnum_classes = df[\"class\"].nunique()\n\n# Separate features and target\nX = df.drop(\"class\", axis=1)\ny = df[\"class\"]\n\n# Initialize and fit the MFE model\nmfe = MFE(\n    groups=[\"general\", \"statistical\", \"info-theory\", \"model-based\", \"landmarking\"]\n)\nmfe.fit(X.values, y.values)\n\n# Extract meta-features\nft = mfe.extract(suppress_warnings=True)\n\n# Convert the meta-features to a DataFrame\nmeta_features_df = pd.DataFrame(ft).T\nmeta_features_df.columns = [\"Feature\", \"Value\"]\n\n# Create a DataFrame for basic features\nbasic_features = pd.DataFrame(\n    {\n        \"Feature\": [\"num_instances\", \"num_attributes\", \"num_classes\"],\n        \"Value\": [num_instances, num_attributes, num_classes],\n    }\n)\n\n# Concatenate basic features with meta-features\nall_features_df = pd.concat([basic_features, meta_features_df], ignore_index=True)\n\n# Save the DataFrame to a CSV file\nall_features_df.to_csv(\"soybean_all_features.csv\", index=False)\n\n# Optional: Print the path to confirm where the file was saved\nprint(\"All features saved to 'soybean_all_features.csv'\")\n\nc:\\Users\\Admin\\miniconda3\\envs\\recsys\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:725: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n  warnings.warn(\nc:\\Users\\Admin\\miniconda3\\envs\\recsys\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\nc:\\Users\\Admin\\miniconda3\\envs\\recsys\\Lib\\site-packages\\pymfe\\_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_entropy'. Will ignore this method. Error message:\nTypeError(\"'&lt;' not supported between instances of 'NoneType' and 'str'\").\n  warnings.warn(\n\n\nAll features saved to 'soybean_all_features.csv'"
  },
  {
    "objectID": "notebooks/hw02.html#file-output",
    "href": "notebooks/hw02.html#file-output",
    "title": "Home Work 2",
    "section": "File Output",
    "text": "File Output\nThe extracted features are stored in the CSV file soybean_all_features.csv. This file contains both the basic dataset characteristics and the extracted meta-features, formatted as Feature, Value pairs.\n\nThis data card provides a quick summary of the dataset and the code used for feature extraction. It’s a useful reference for understanding the dataset’s structure and the extracted meta-features."
  },
  {
    "objectID": "reference/create_pipeline.html",
    "href": "reference/create_pipeline.html",
    "title": "create_pipeline",
    "section": "",
    "text": "create_pipeline\ncreate_pipeline(**kwargs)\nCreates a data processing pipeline.\nThis pipeline consists of a single node that preprocesses the data from the input dataset.\nParameters: **kwargs: Additional keyword arguments.\nReturns: Pipeline: A Kedro pipeline object with the defined nodes."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Documentation for HW03",
    "section": "",
    "text": "Srinivasan-ai-839\n\n\n\n_isTelephone\nChecks if the values in the given pandas Series are equal to “yes”.\n\n\npreprocess_data\nConverts all object columns in the given DataFrame to categorical values using one-hot encoding.\n\n\nevaluate_model\nEvaluates the trained model using the testing data and logs the accuracy.\n\n\nsplit_dataset\nSplits the dataset into training and testing sets for features and target.\n\n\ntrain_model\nTrains a RandomForestClassifier model using the provided training data.\n\n\ncreate_pipeline\nCreates a data processing pipeline."
  },
  {
    "objectID": "reference/index.html#srinivasan_ai_839",
    "href": "reference/index.html#srinivasan_ai_839",
    "title": "Documentation for HW03",
    "section": "",
    "text": "Srinivasan-ai-839\n\n\n\n_isTelephone\nChecks if the values in the given pandas Series are equal to “yes”.\n\n\npreprocess_data\nConverts all object columns in the given DataFrame to categorical values using one-hot encoding.\n\n\nevaluate_model\nEvaluates the trained model using the testing data and logs the accuracy.\n\n\nsplit_dataset\nSplits the dataset into training and testing sets for features and target.\n\n\ntrain_model\nTrains a RandomForestClassifier model using the provided training data.\n\n\ncreate_pipeline\nCreates a data processing pipeline."
  },
  {
    "objectID": "reference/split_dataset.html",
    "href": "reference/split_dataset.html",
    "title": "split_dataset",
    "section": "",
    "text": "split_dataset\nsplit_dataset(dataset)\nSplits the dataset into training and testing sets for features and target.\nParameters: dataset (pd.DataFrame): The dataset to be split.\nReturns: t.Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: A tuple containing the training features, testing features, training target, and testing target."
  }
]